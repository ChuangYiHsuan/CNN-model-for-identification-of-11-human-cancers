import os
os.environ ["CUDA_VISIBLE_DEVICES"] = "0" # assign specific graphic card (0 to 3)
import warnings
os.environ ["TF_CPP_MIN_LOG_LEVEL"] = "3" # ignore warnings about version of python or tensorflow
warnings.filterwarnings('ignore') # "error", "ignore", "always", "default", "module" or "once"
import sys
from datetime import datetime as DT
import numpy as np

import matplotlib
import matplotlib.pyplot as plt
matplotlib.use("Agg") # save files, not print on screen

import tensorflow as tf
from keras.utils import np_utils
from keras.models import Sequential # build CNN model
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D
from keras.layers import BatchNormalization as B_nor
from tensorflow.keras.optimizers import Adam
from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, Callback
import keras.backend as K
from sklearn.metrics import f1_score, precision_score, recall_score, matthews_corrcoef, roc_auc_score

# plot 100 epoch training results
def save_train_history(train_history, train, validation, name):
    plt.plot(train_history.history[train])
    plt.plot(train_history.history[validation])
    plt.title('Train History')
    plt.ylabel(train)
    plt.xlabel('Epoch')
    plt.legend(['train', 'validation'], loc = 'upper left')
    name = name + ".png"
    plt.savefig(name, bbox_inches = "tight")
    plt.close()

# Matthews correlation coefficient (multiple classification)
def MCC(y_true, y_pred):
    y_pred_pos = K.round(K.clip(y_pred, 0, 1))
    y_pred_neg = 1 - y_pred_pos

    y_pos = K.round(K.clip(y_true, 0, 1))
    y_neg = 1 - y_pos

    tp = K.sum(y_pos * y_pred_pos)
    tn = K.sum(y_neg * y_pred_neg)

    fp = K.sum(y_neg * y_pred_pos)
    fn = K.sum(y_pos * y_pred_neg)

    numerator = (tp * tn - fp * fn)
    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))

    return numerator / (denominator + K.epsilon())

# 1. Load training data and its label
# 2. Reshape input size 100*100
training_set = sys.argv[1] # training sample (one npy file containing all training data, npy file was generated by Step1_GenerateNpyFile.py)
x_sample = np.load(training_set)
x_sample = x_sample.reshape((x_sample.shape[0], 100, 100,1)) # sample size, height, width, 1 channel (because not RGB)

training_label = sys.argv[2] # label of training data
y_label = np.load(training_label)
y_label = np_utils.to_categorical(y_label)
out_file_name=sys.argv[3]
num_epochs=100

#Build CNN model (Keras)
model = Sequential()
# Conv 1: frame 5*5, kernel number=64, input size 100*100
model.add(Conv2D(filters = 64, kernel_size = (5, 5), input_shape = (100, 100, 1)))
model.add(B_nor(axis = 2, epsilon = 1e-5)) # layer of batch normalization, normalize value to accelerate learning (accelerate convergence, avoid gradient vanishing/exploding )
model.add(MaxPooling2D(pool_size = (2, 2), padding = "same")) # same: padding, "same" size of input
# Conv 2
model.add(Conv2D(filters = 64, kernel_size = (3, 3)))
model.add(B_nor(axis = 2, epsilon = 1e-5))
model.add(MaxPooling2D(pool_size = (2, 2), padding = "same"))
# Conv 3
model.add(Conv2D(filters = 64, kernel_size = (3, 3)))
model.add(B_nor(axis = 2, epsilon = 1e-5))
model.add(MaxPooling2D(pool_size = (2, 2), padding = "same"))

# Fully connect
model.add(Flatten())

# Hidden layers
model.add(Dense(1000, activation = "relu")) # 1st Hidden layer, 1000 neuron nodes, ReLU (Rectified Linear Unit)
model.add(Dense(600, activation = "relu")) # 2nd layer
model.add(Dense(80, activation = "relu")) # 3rd layer
model.add(Dense(12, activation = "softmax")) # 12 label classes, 11 cancers + 1 normal type

#Executing model with training npy file
optimizer_Adam = Adam(lr = 1e-4) # learning rate
#Executing = model.compile()
#loss function = categorical_crossentropy (multiple classification, use for one-hot encoding, 12 unique types)
#optimizer = Adam
#performance = MCC
model.compile(loss = 'categorical_crossentropy', optimizer = optimizer_Adam, metrics = ['accuracy', MCC]) # metrics = ['accuracy', MCC], automaitcally call ACC, MCC function

#Start training *
# this step return "training history"
train_history = model.fit(x_sample, y_label, # training sample and label
                                              validation_split = 0.25, # 25% sample for cross-validation
                                              epochs = num_epochs, #epochs = num_epochs = 100
                                              batch_size = 24, # each batch including 24 samples, to update parameters
                                              verbose = 1)
out = open(out_file_name+"_tranning_message", 'w')
step=0
for i in train_history.history['accuracy']: # write out ACC for 100 epochs
	print('Epoch:\t%d/%d\tLoss:\t%f\tAcc:\t%f\tMCC:\t%f\tVal_Loss:\t%f\tVal_Acc:\t%f\tVal_MCC:\t%f' % (step+1, num_epochs, 
		train_history.history['loss'][step], i, train_history.history['MCC'][step], train_history.history['val_loss'][step], 
		train_history.history['val_accuracy'][step], train_history.history['val_MCC'][step]), file=out)
	step +=1
out.close()

# Saving training process
final_model = out_file_name+".save"
model.save_weights(final_model)

# Saving graph of loss, acc, val_loss, val_acc *
save_train_history(train_history, "accuracy", "val_accuracy", out_file_name+"-acc")
save_train_history(train_history, "loss", "val_loss", out_file_name+"-loss")
model.summary()
